---
title: 【机器学习】激活函数
date: 2017-10-25 18:35:06
tags:
    - machine leaning
    - pattern recognition
    - deep learning]
categories: 【机器学习】
---

# 0 引言
常见的激活函数有sigmoid, tanh, relu等。
sigmoid: $f(x)=\frac{1}{1+exp(-x)}$，导数：$f(x)^{\prime}=f(x)(1-f(x))$

tanh: $f(x)=tanh(x)$，导数：$f(x)^{\prime}=1-(f(x))^2$

relu: $f(x)=\max(0,x)$，导数：$f(x)^{\prime}=\max(0,1)$

<!--more-->

# 1 梯度消失、梯度发散、梯度爆炸
这种问题表现为，越靠后的隐含层收敛速度越快，前面隐含层可能不会收敛，甚至发散。
原因是激活函数在求导的时候，如果很小，那么在误差反向传播过程中，由于链式法则，越靠近前面的隐含层参数的导数就越小，学习的效率就越低。如果传到前面，导数已经下降到零。此时就会导致前面隐含层的参数不会更新。

尤其是sigmoid函数和tanh函数，sigmoid函数的导数，在0处取最大值为0.25，tanh函数的导数在0处取最大值为1，当连续相乘多次的时候，必然会越来越小。那么解决梯度消失的方法是使用relu。

# 2 为什么梯度方向下降最快？
对二元函数$f(x,y)$进行泰勒展开，得到下面的公式
$$
f(x+\Delta x, y+\Delta y) - f(x,y) \approx \[\frac{\partial f(x,y)}{\partial x},\frac{\partial f(x,y)}{\partial y}\]\[\Delta x, \Delta y\]^T
$$

按照$\[\Delta x, \Delta y\]$方向走固定大小的距离，函数的衰减值越大，下降越快。那么将梯度向量和变化向量全部化成单位向量，提出一个常量，那么当行走方向和梯度方向重合的时候，变化最大。也就是下降最快。

# 3 


# 参考文献

