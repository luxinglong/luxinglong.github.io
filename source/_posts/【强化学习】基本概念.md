---
title: 【强化学习】基本概念
date: 2017-10-19 10:02:14
tags:
    - RL
    - robotics
categories: 【强化学习】
---
# 0 引言
项目需要，所以学习强化学习。强化学习以及深度强化学习可以使机器人变得更加智能。
总体的学习路线
1. 理论部分学习
首先学习强化学习的基本概念，然后慢慢学习强化学习解决问题的思路，最后学习强化学习的基本处理方法。
2. 实践部分
主要开展基于强化学习环境OpenAI gym和深度学习框架mxnet和tensorflow的仿真试验。
首先，研究几个案例，包括倒立摆模型和找金币棋盘游戏，学习他们的环境和学习策略。然后，对避障算法进行建模，并设计仿真环境。最后，避障算法进行策略学习和试验。

<!--more-->
# 1 强化学习的基本概念
{% img [rl] http://on99gq8w5.bkt.clouddn.com/rl.png?imageMogr2/thumbnail/300x200 %}
强化学习作为一个学习的问题，就是通过最大化长远奖励来学习控制一个系统。强化学习的框架可以表示成上图的形式，一个控制器接受到被控制系统的状态和最新的奖励，然后按照学习到的策略给系统一个动作。同时，系统转化到一个新的状态并给出一个奖赏值。如此反复，达到控制的目的。

一个关于强化学习的智能体要包含策略、值函数和环境模型三个部分。
策略：是状态空间到动作空间的映射，即$\pi:(x)=a$
值函数：未来回报值的期望，用来衡量当前状态的好坏 $

模型：环境状态的转移概率分布

智能体和环境：
智能体的状态：$S^{a}_{t}$智能体内部的状态，用来决定自己怎么采取下一步的动作
环境的状态：$S^{e}_{t}$

马尔可夫状态：当$S_{t}$满足，$\mathbb{P}[S_{t+1}|S_{t}]=\mathbb{P}[S_{t+1}|S_{1},\cdots,S_{t}]$


预测和控制： 
prediction: 评估未来，在给定的策略下 策略评估
control: 优化未来，选出最好的策略    策略改进： 策略迭代、值迭代

最优值函数：
$$
v_{\ast}(x)=\max \limits_{\pi}v_{\pi}(x) \\
q_{\ast}(x,a)=\max \limits_{\pi}q_{\pi}(x,a)
$$
MDP或者说强化学习的目标就是获得最优值函数。因为有了值函数，便可以得到最优策略。
最优策略：
首先，怎么定义一个策略比另外一个策略好呢？
$$
\pi \ge \pi^{\prime}, if\quad v_{\pi}(x) \ge v_{\pi^{\prime}}(x), \forall x
$$

然后，MDP的性质：
* 一个MDP问题，至少存在一个最优策略$\pi_{\star}$，好于或等于其他策略$\pi_{\ast} \ge \pi, \ge \pi$
* 所有的最优策略都实现最优的值函数

最后，在最优值函数的前提下，如何获得最优策略呢？最大化动作-状态值函数。
$$
\[
\pi_{\ast}(a\mid x)=
\begin{cases}
1& \max \limits_{a \in A}q_{\ast}(x, a)\\
0& \text{otherwise}
\end{cases}
\]
$$

既然说所有的MDP问题，都是在求最优值函数，那么如何求解呢？这里就需要引入最优Bellman等式。
$$
v_{\ast}(x)=\max \limits_a q_{\ast}(x,a) \\
q_{\ast}(x,a)=R^a_x+\gamma sum_{x^{\prime}\in X}P^a_{ss^{\prime}}v_{\ast}(x^{\prime}) \\
v_{\ast}=\max \limits_a R^a_x+\gamma sum_{x^{\prime}\in X}P^a_{xx^{\prime}}v_{\ast}(x^{\prime}) \\
q_{\ast}(x,a)=R^a_x+\gamma sum_{x^{\prime}\in X}P^a_{xx^{\prime}} \max \limits_{a^{\prime}}q_{\ast}(x^{\prime},a^{\prime})
$$

在最优Bellman等式的基础上，可以采用迭代的方式求解，如值迭代、策略迭代、Q-learning和Sarsa等方法。

# 2 为什么要加“深度”？

因为一般的问题，状态空间和动作空间都是离散的，可以通过简单的映射关系找到值函数，但是对于连续的动作空间


# 3 参考文献
[1] David Silver, reinforcement learning lecture 2