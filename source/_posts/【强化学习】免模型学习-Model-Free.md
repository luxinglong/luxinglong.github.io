---
title: 【强化学习】免模型学习(Model-Free)
date: 2017-10-20 18:57:59
tags:
    - RL
    - robotics
categories: 【强化学习】
---

# 0 引言
在现实的强化学习任务中，环境的模型，也就是状态转移概率、奖赏函数不易得知，因此需要进行免模型学习。

<!--more-->

# 1 估计
在没有环境模型的情形下，策略迭代面临两大困难：一是无法对一个给定的策略进行评估，因为P、R未知，无法得到Bellman等式。二是无法建立状态值函数和动作-状态值函数的联系。(策略迭代算法估计的是最优状态值函数，而最优策略要由最优动作-状态值函数获得)
## 1.1 蒙特卡罗
对于问题一，蒙特卡罗采用采样的办法，即通过在环境中执行动作，观察环境的状态转移和得到的奖赏，经过多次“采样”，求取平均累积奖赏作为期望累积奖赏。对于问题二，蒙特卡罗绕过状态值函数，直接估计动作-状态值函数。

策略评估：

轨迹：$\langle x_0,a_0,r_1,x_1,a_1,r_2,\cdots,x_{T-1},a_{T-1},r_T,x_T\rangle$

策略改进：

同策略蒙特卡罗强化学习算法：


## 1.2 时序差分
蒙特卡罗方法效率较低，而时序差分结合了蒙特卡罗和动态规划的思想。

# 2 优化

#  参考文献
[1] 周志华,《机器学习》,清华大学出版社,2016
[2] David Silver, reinforcement learning lecture 4 and 5
