---
title: 【机器学习】激活函数
date: 2017-10-25 18:35:06
tags:
    - machine leaning
    - pattern recognition
    - deep learning]
categories: 【机器学习】
---

# 0 引言
常见的激活函数有sigmoid, tanh, relu等。
sigmoid: $f(x)=\frac{1}{1+exp(-x)}$，导数：$f(x)^{\prime}=f(x)(1-f(x))$

tanh: $f(x)=tanh(x)$，导数：$f(x)^{\prime}=1-(f(x))^2$

relu: $f(x)=\max(0,x)$，导数：$f(x)^{\prime}=\max(0,1)$

<!--more-->

# 1 梯度消失、梯度发散、梯度爆炸
这种问题表现为，越靠后的隐含层收敛速度越快，前面隐含层可能不会收敛，甚至发散。
原因是激活函数在求导的时候，如果很小，那么在误差反向传播过程中，由于链式法则，越靠近前面的隐含层参数的导数就越小，学习的效率就越低。如果传到前面，导数已经下降到零。此时就会导致前面隐含层的参数不会更新。

尤其是sigmoid函数和tanh函数，sigmoid函数的导数，在0处取最大值为0.25，tanh函数的导数在0处取最大值为1，当连续相乘多次的时候，必然会越来越小。那么解决梯度消失的方法是使用relu。

# 2 为什么梯度方向下降最快？
对二元函数$f(x,y)$进行泰勒展开，得到下面的公式
$$
f(x+\Delta x, y+\Delta y) - f(x,y) \approx \[\frac{\partial f(x,y)}{\partial x},\frac{\partial f(x,y)}{\partial y}\]\[\Delta x, \Delta y\]^T
$$

按照$\[\Delta x, \Delta y\]$方向走固定大小的距离，函数的衰减值越大，下降越快。那么将梯度向量和变化向量全部化成单位向量，提出一个常量，那么当行走方向和梯度方向重合的时候，变化最大。也就是下降最快。

# 3 当目标函数非凸时，应该采用什么优化方法？
凸优化：一个问题是凸优化问题，必须满足两个条件：
1. 目标函数为凸函数
2. 由约束条件形成的定义域为凸集

如果有其中一个条件不满足，那么则称为非凸优化。

如果一个问题是凸优化问题，那么可以采用梯度下降等方法，得到的局部最优，也是全局最优。而如果一个问题是非凸优化，一般会有两种解决办法。一种是将非凸优化问题转化成凸优化问题。
1. 将目标函数处理成凸函数
2. 放宽约束条件的限制，使之成为凸集

另外一种是使用非凸优化的方法，如蒙特卡罗方法，找出所有的局部最优，从中选出全局最优。这种方法计算量很大。


# 参考文献

